{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [範例重點]\n",
    "了解機器學習建模的步驟、資料型態以及評估結果等流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "\n",
    "# 如果是分類問題，請使用 DecisionTreeClassifier，若為回歸問題，請使用 DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型四步驟\n",
    "\n",
    "在 Scikit-learn 中，建立一個機器學習的模型其實非常簡單，流程大略是以下四個步驟\n",
    "\n",
    "1. 讀進資料，並檢查資料的 shape (有多少 samples (rows), 多少 features (columns)，label 的型態是什麼？)\n",
    "    - 讀取資料的方法：\n",
    "        - **使用 pandas 讀取 .csv 檔：**pd.read_csv\n",
    "        - **使用 numpy 讀取 .txt 檔：**np.loadtxt \n",
    "        - **使用 Scikit-learn 內建的資料集：**sklearn.datasets.load_xxx\n",
    "    - **檢查資料數量：**data.shape (data should be np.array or dataframe)\n",
    "2. 將資料切為訓練 (train) / 測試 (test)\n",
    "    - train_test_split(data)\n",
    "3. 建立模型，將資料 fit 進模型開始訓練\n",
    "    - clf = DecisionTreeClassifier()\n",
    "    - clf.fit(x_train, y_train)\n",
    "4. 將測試資料 (features) 放進訓練好的模型中，得到 prediction，與測試資料的 label (y_test) 做評估\n",
    "    - clf.predict(x_test)\n",
    "    - accuracy_score(y_test, y_pred)\n",
    "    - f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取鳶尾花資料集\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立模型\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# 訓練模型\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy:  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Acuuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:  [0.         0.01796599 0.05992368 0.92211033]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature importance: \", clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier 參數\n",
    "\n",
    "class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### criterion:\n",
    "\n",
    "DecisionTreeClassifier\n",
    "特征选择标准，默认值为'gini'。（ gini, entropy）前者代表基尼係數，後者代表信息增益。\n",
    "一般說使用默認的基尼係數\"gini\"就可以了，即CART算法。\n",
    "\n",
    "DecisionTreeRegressor\n",
    "可以使用\"mse\"或者\"mae\"，前者是均方差，后者是和均值之差的绝对值之和。推荐使用默认的\"mse\"。一般来说\"mse\"比\"mae\"更加精确。除非你想比较二个参数的效果的不同之处。\n",
    "\n",
    "### splitter:\n",
    "          \n",
    "特征划分标准，默认值为‘best’。（best, random）\n",
    "\n",
    "best在特征的所有划分点中找出最优的划分点，random 随机的在部分划分点中找局部最优的划分点。\n",
    "\n",
    "默认的‘best’适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐‘random’。\n",
    "\n",
    "### max_depth:\n",
    "             \n",
    "决策树最大深度。默认值是'None'。（int,  None）\n",
    "\n",
    "一般数据比较少或者特征少的时候可以不用管这个值，如果模型样本数量多，特征也多时，推荐限制这个最大深度,具体取值取决于数据的分布。常用的可以取值10-100之间，常用来解决过拟合。\n",
    "\n",
    "### min_samples_split:\n",
    "\n",
    "内部节点再划分所需最小样本数。默认值为2。（int, float）\n",
    "\n",
    "如果是int，则取传入值本身作为最小样本数；如果是float，则取ceil(min_samples_split*样本数量)作为最小样本数。（向上取整）\n",
    "这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。\n",
    "\n",
    "### min_samples_leaf:\n",
    "\n",
    "叶子节点最少样本数。默认值为1。\n",
    "\n",
    "如果是int，则取传入值本身作为最小样本数；如果是float，则取ceil(min_samples_leaf*样本数量)的值作为最小样本数。这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。\n",
    "這個值限制了葉子節點最少的樣本數，如果某葉子節點數目小於樣本數，則會和兄弟節點一起被剪枝。 默認是1,可以輸入最少的樣本數的整數，或者最少樣本數佔樣本總數的百分比。如果樣本量不大，不需要管這個值。如果樣本量數量級非常大，則推薦增大這個值。之前的10萬樣本項目使用min_samples_leaf的值為 5，僅供參考。\n",
    "\n",
    "### min_weight_fraction_leaf:\n",
    "\n",
    "叶子节点最小的样本权重和。默认为0。（float）\n",
    "\n",
    "这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。\n",
    "\n",
    "默认是0，就是不考虑权重问题,所有样本的权重相同。一般来说如果我们有较多样本有缺失值或者分类树样本的分布类别偏差很大，就会引入样本权重，这时就要注意此值。\n",
    "\n",
    "### max_features:\n",
    "\n",
    "在划分数据集时考虑的最多的特征值数量。\n",
    "\n",
    "int值，在每次split时最大特征数；float值表示百分数，即（max_features*n_features）;\n",
    "\n",
    "可以使用很多种类型的值，默认是\"None\",意味着划分时考虑所有的特征数；如果是\"log2\"意味着划分时最多考虑$log_2N$个特征；如果是\"sqrt\"或者\"auto\"意味着划分时最多考虑$\\sqrt{N}$个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。\n",
    "\n",
    "一般来说，如果样本特征数不多，比如小于50，我们用默认的\"None\"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。\n",
    "\n",
    "### random_state:\n",
    "\n",
    "默认是None，（int, randomSate instance, None）\n",
    "\n",
    "### max_leaf_nodes:\n",
    "\n",
    "最大叶子节点数。默认为None。（int, None）\n",
    "\n",
    "通过设置最大叶子节点数，可以防止过拟合，默认情况下是不设置最大叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征多，可以加限制，具体的值可以通过交叉验证得到。\n",
    "\n",
    "### min_impurity_decrease:\n",
    "\n",
    "加权不纯度的减少量 float, optional (default=0.)\n",
    "\n",
    "如果节点的分裂导致不纯度的减少(分裂后样本比分裂前更加纯净)大于或等于 min_impurity_decrease，则分裂该节点。\n",
    "个人理解这个参数应该是针对分类问题时才有意义。这里的不纯度应该是指基尼指数。回归生成树采用的是平方误差最小化策略。分类生成树采用的是基尼指数最小化策略。\n",
    "加权不纯度的减少量计算公式为：\n",
    "min_impurity_decrease=N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)\n",
    "其中N是样本的总数，N_t是当前节点的样本数，N_t_L是分裂后左子节点的样本数，N_t_R是分裂后右子节点的样本数。impurity 指当前节点的基尼指数，right_impurity 指分裂后右子节点的基尼指数。left_impurity 指分裂后左子节点的基尼指数。\n",
    "\n",
    "\n",
    "### min_impurity_split:\n",
    "\n",
    "节点划分最小不纯度\n",
    "\n",
    "这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。\n",
    "\n",
    "### class_weight:\n",
    "\n",
    "类别权重。默认为 None，(dict, list of dicts, balanced)\n",
    "\n",
    "指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重，或者用“balanced”，如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数，选择默认的\"None\"\n",
    "**不适用于回归树，sklearn.tree.DecisionTreeRegressor.**\n",
    "\n",
    "### presort:\n",
    "\n",
    "數據是否預排序\n",
    "\n",
    "bool，默认是False，表示在进行拟合之前，是否预分数据来加快树的构建。对于数据集非常庞大的分类，presort=true将导致整个分类变得缓慢；当数据集较小，且树的深度有限制，presort=true才会加速分类。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了这些参数要注意以外，其他在调参时的注意点有：\n",
    "\n",
    "1）当样本少数量但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型\n",
    "\n",
    "2）如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。\n",
    "\n",
    "3）推荐多用决策树的可视化（下节会讲），同时先限制决策树的深度（比如最多3层），这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。\n",
    "\n",
    "4）在训练模型先，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。\n",
    "\n",
    "5）决策树的数组使用的是 numpy 的 float32 类型，如果训练数据不是这样的格式，算法会先做 copy 再运行。\n",
    "\n",
    "6）如果输入的样本矩阵是稀疏的，推荐在拟合前调用 csc_matrix 稀疏化，在预测前调用 csr_matrix 稀疏化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "1. 試著調整 DecisionTreeClassifier(...) 中的參數，並觀察是否會改變結果？\n",
    "2. 改用其他資料集 (boston, wine)，並與回歸模型的結果進行比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
